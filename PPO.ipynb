{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0vrcXPde4Tcs",
        "swI8tn4Z4jgh",
        "h2ADMYKA4vmK",
        "6gUrD7azUQ9J",
        "UR3xJqSvU73y",
        "lGWninDCVNJH",
        "sQcwOGwq3iu2",
        "AKMwBRCmTBSm",
        "Cd8wVqKNTGuy",
        "dL14jbtRSi6h"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise Overview\n",
        "\n",
        "Implementation of PPO to solve the bipedal walker from Gymnasium environment\n"
      ],
      "metadata": {
        "id": "G6KRFo0DTgRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install & Import Requirements\n",
        "\n"
      ],
      "metadata": {
        "id": "0vrcXPde4Tcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "rrswUBWbwgYC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c075ecb7-48ff-4e7b-9b3a-d89f4de98050"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (3.5 kB)\n",
            "Downloading swig-4.4.1-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.6/1.9 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.4.1\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.3)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d==2.3.10 (from gymnasium[box2d])\n",
            "  Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (573 bytes)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.1)\n",
            "Downloading Box2D-2.3.10-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: box2d\n",
            "Successfully installed box2d-2.3.10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from typing import Sequence\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "vSTbNBUqvggp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "7CQCDS-pTWa8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP function to create the policy network of our agent"
      ],
      "metadata": {
        "id": "swI8tn4Z4jgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mlp(sizes: Sequence[int], activation=nn.ReLU, output_activation=nn.Identity) -> nn.Sequential:\n",
        "  \"\"\"\n",
        "      Create a simple feedforward neural network.\n",
        "  \"\"\"\n",
        "  layers = []\n",
        "  for j in range(len(sizes)-1):\n",
        "    act = activation if j < len(sizes)-2 else output_activation\n",
        "    layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "  return nn.Sequential(*layers)"
      ],
      "metadata": {
        "id": "e9lecbV0xuQY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to get the ouput of the policy network"
      ],
      "metadata": {
        "id": "h2ADMYKA4vmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_policy(obs: torch.Tensor) -> Normal:\n",
        "  \"\"\"\n",
        "  Get the stochastic policy for a given observation (-batch).\n",
        "  Returns a distribution for every action-dimension.\n",
        "  \"\"\"\n",
        "  obs = obs.unsqueeze(0) if obs.dim() == 1 else obs  # for single observations that do not have a batch dimension\n",
        "  logits = actor(obs)\n",
        "  mean, logstd = logits[:, :n_acts], logits[:, n_acts:]  # split the output layer into mean and logstd\n",
        "  logstd = torch.clamp(logstd, min=-20, max=2)  # for numerical stability\n",
        "  return Normal(mean, torch.exp(logstd))"
      ],
      "metadata": {
        "id": "7znaERjC2lwZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(obs: torch.Tensor) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Get the action (-batch) from the policy for a given observation (-batch).\n",
        "  \"\"\"\n",
        "  dist = get_policy(obs)\n",
        "  return dist.sample().squeeze(0).numpy()"
      ],
      "metadata": {
        "id": "K-f7TYWJ2nDo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PPO, we need to compute log probabilities separately for importance sampling.\n",
        "\n",
        "This function extracts the log probability computation."
      ],
      "metadata": {
        "id": "-7pSoO8tUekq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_log_prob(obs: torch.Tensor, act: torch.Tensor) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Get the log-probability of a given action for a given observation.\n",
        "\n",
        "  Args:\n",
        "      obs: Observations tensor of shape (batch_size, obs_dim)\n",
        "      act: Actions tensor of shape (batch_size, n_acts)\n",
        "\n",
        "  Returns:\n",
        "      log_probs: Log probabilities of shape (batch_size,)\n",
        "  \"\"\"\n",
        "  dist = get_policy(obs)\n",
        "  logp = dist.log_prob(act).sum(dim=-1) #sum actions up fro example prob of left thtrust and right thrust\n",
        "  return logp"
      ],
      "metadata": {
        "id": "Lv3Tj8j1UZof"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to compute GAE"
      ],
      "metadata": {
        "id": "sQcwOGwq3iu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gae(rewards: torch.Tensor,\n",
        "                values: torch.Tensor,\n",
        "                next_values: torch.Tensor,\n",
        "                dones: torch.Tensor,  # values are either 1.0 (has ended) or 0.0 (has not ended), indicating whether an episode has ended or not.\n",
        "                gamma=0.99,  # discount factor (0,1]\n",
        "                lam=0.95,  # trace-decay parameter [0,1]. lam=0.0: temporal difference, lam=1.0: Monte Carlo\n",
        "                ) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Compute the Generalized Advantage Estimation (GAE) used for the actor loss function.\n",
        "  Balances the bias-variance tradeoff of the advantage estimates.\n",
        "  lam=0.0: temporal difference, high bias - low variance\n",
        "  lam=1.0: Monte Carlo, high variance - low bias\n",
        "  \"\"\"\n",
        "  T = rewards.shape[0]\n",
        "  advantages = torch.zeros_like(rewards)\n",
        "  gae = 0.0\n",
        "  for t in reversed(range(T)):\n",
        "    nonterminal = 1.0 - dones[t]\n",
        "    delta = rewards[t] + gamma * next_values[t] * nonterminal - values[t]\n",
        "    gae = delta + gamma * lam * nonterminal * gae\n",
        "    advantages[t] = gae\n",
        "  return advantages"
      ],
      "metadata": {
        "id": "rGa1xIfn3nAX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO uses a clipped surrogate objective instead of vanilla policy gradient.\n",
        "\n",
        "Key components:\n",
        "1. Importance sampling ratio (for numerical stability we first calclate the log of the ratio)\n",
        "2. Clipped ratio: prevents ratio from going too far from 1.0\n",
        "3. Clipped objective\n",
        "4. Entropy bonus: encourages exploration\n"
      ],
      "metadata": {
        "id": "qvqbfILFV04J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_actor_loss(obs: torch.Tensor,\n",
        "                  act: torch.Tensor,\n",
        "                  weights: torch.Tensor,\n",
        "                  old_logp: torch.Tensor,\n",
        "                  beta: float = 0.00  # no entropy regularization per default\n",
        "                  ) -> torch.Tensor:\n",
        "  \"\"\"\n",
        "  Compute the PPO clipped surrogate loss with entropy regularization for the actor.\n",
        "  weights: advantages\n",
        "  old_logp: log-probability under the old policy (before doing a first update)\n",
        "  beta: hyperparameter for entropy regularization\n",
        "  \"\"\"\n",
        "  dist = get_policy(obs)\n",
        "  logp = get_log_prob(obs, act)\n",
        "  ratio = torch.exp(logp - old_logp)  # importance weight\n",
        "  clipped_obj = torch.min(ratio * weights, torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * weights).mean()\n",
        "  entropy = dist.entropy().sum(dim=-1).mean()\n",
        "\n",
        "  return - clipped_obj - beta * entropy"
      ],
      "metadata": {
        "id": "cAzIijKr2okX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function to compute the discounted return"
      ],
      "metadata": {
        "id": "Cd8wVqKNTGuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discounted_return(arr: Sequence[float], gamma=0.99) -> list[float]:\n",
        "  \"\"\"\n",
        "  Compute the discounted return for a single episode, given a sequence of rewards.\n",
        "  gamma: discount factor (0,1]\n",
        "  Used for the MSE loss function of the critic.\n",
        "  \"\"\"\n",
        "  ret = [0.0] * len(arr)\n",
        "  ret[-1] = arr[-1]\n",
        "  for i in range(len(arr)-2, -1, -1):\n",
        "    ret[i] = arr[i] + gamma * ret[i+1]\n",
        "  return ret"
      ],
      "metadata": {
        "id": "4oyDxrHn2q_v"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PPO makes several key changes to Actor-Critic's training:\n",
        "\n",
        "1. Compute old_log_prob BEFORE any updates (for importance sampling)\n",
        "2. Remove separate critic update loop\n",
        "3. Add outer loop for multiple epochs (n_ppo_epochs)\n",
        "4. Add inner loop for mini-batches with shuffling\n",
        "5. Both actor and critic update in each mini-batch\n",
        "6. Add gradient clipping for stability"
      ],
      "metadata": {
        "id": "MqQVo_TOXZJ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch() -> tuple[list, list]:\n",
        "  \"\"\"\n",
        "  Train the actor and critic for one epoch,\n",
        "  i.e. one actor-update and n_critic_updates critic-updates.\n",
        "  \"\"\"\n",
        "  batch_obs = []\n",
        "  batch_acts = []\n",
        "  batch_rewards = []\n",
        "  batch_next_obs = []\n",
        "  batch_dones = []\n",
        "  batch_rets = []\n",
        "  batch_lens = []\n",
        "  batch_Rtogo = []\n",
        "\n",
        "  obs, _ = env.reset()\n",
        "  ep_rews = []\n",
        "\n",
        "  while True:\n",
        "    act = get_action(torch.as_tensor(obs, dtype=torch.float32))\n",
        "    next_obs, rew, terminated, truncated, _ = env.step(act)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    batch_obs.append(obs.copy())  # copy as obs is modified in-place\n",
        "    batch_acts.append(act)  # act is newly initialized every loop -> no copy\n",
        "    batch_rewards.append(rew)\n",
        "    batch_next_obs.append(next_obs.copy())\n",
        "    batch_dones.append(done)\n",
        "    ep_rews.append(rew)\n",
        "\n",
        "    obs = next_obs\n",
        "\n",
        "    if done:\n",
        "      batch_rets.append(sum(ep_rews))\n",
        "      batch_lens.append(len(ep_rews))\n",
        "      batch_Rtogo += discounted_return(ep_rews, gamma=gamma)\n",
        "      ep_rews = []\n",
        "      obs, _ = env.reset()\n",
        "\n",
        "      if len(batch_obs) > batch_size:\n",
        "        break\n",
        "\n",
        "  # convert lists to tensors\n",
        "  batch_obs = torch.as_tensor(np.array(batch_obs), dtype=torch.float32)\n",
        "  batch_next_obs = torch.as_tensor(np.array(batch_next_obs), dtype=torch.float32)\n",
        "  batch_acts = torch.as_tensor(np.array(batch_acts), dtype=torch.float32)\n",
        "  batch_rewards = torch.as_tensor(np.array(batch_rewards), dtype=torch.float32)\n",
        "  batch_dones = torch.as_tensor(np.array(batch_dones), dtype=torch.float32)\n",
        "  batch_Rtogo = torch.as_tensor(np.array(batch_Rtogo), dtype=torch.float32)\n",
        "\n",
        "  V_target = batch_Rtogo.detach()  # target for critic, cannot have gradients\n",
        "\n",
        "  # Remove the critic update here, remember that PPO is introduced to speed up the computational time\n",
        "  # we can witness the fact that for just 1 actor update is required to generate several samples and\n",
        "  #and update the critic up to 80 times\n",
        "  #now we want with the same batch of observations to train both the actor and the critic multiple times\n",
        "\n",
        "  # calculate generalized advantage estimate GAE\n",
        "  with torch.no_grad():  # advantages should never have gradients\n",
        "    value = critic(batch_obs).squeeze()\n",
        "    next_values = critic(batch_next_obs).squeeze()\n",
        "    A_gae = compute_gae(batch_rewards, value, next_values, batch_dones, gamma=gamma, lam=lam)  # weight for actor loss function\n",
        "  A_gae = ((A_gae - A_gae.mean()) / (A_gae.std() + 1e-8))\n",
        "\n",
        "\n",
        "  # Compute old_log_prob before any updates (for importance sampling)\n",
        "  old_log_prob = get_log_prob(batch_obs, batch_acts).detach()\n",
        "\n",
        "\n",
        "  #Add outer loop for multiple epochs (use n_ppo_epochs)\n",
        "  #Inside outer loop, shuffle the data\n",
        "  #Use torch.randperm(len(batch_obs)) to get shuffled indices\n",
        "  #we get shuffled indices in order to have less correlated samples and make data look more iid otherwise sgd doesnt work\n",
        "  for _ in range(n_ppo_epochs):\n",
        "    idx = torch.randperm(len(batch_obs))\n",
        "    #torch.randperm(4) --> tensor([2, 1, 0, 3])\n",
        "    #Add inner loop for mini-batches\n",
        "  #Loop from 0 to len(batch_obs) with step size mini_batch_size\n",
        "  #Get mini-batch indices: idx = indices[start:end]\n",
        "  #Check if idx is not empty before processing\n",
        "    for start in range(0, len(batch_obs), mini_batch_size):\n",
        "      end = min(start + mini_batch_size, len(batch_obs)) #avoid to go over the lengt of the array idx\n",
        "      mini_batch_indices = idx[start:end]\n",
        "\n",
        "      if len(mini_batch_indices) == 0:\n",
        "        continue\n",
        "      #Extract mini-batches for obs, acts, advantages, old_logp, V_target\n",
        "      obs_min_batch = batch_obs[mini_batch_indices]\n",
        "      acts_min_batch = batch_acts[mini_batch_indices]\n",
        "      old_logp_min_batch = old_log_prob[mini_batch_indices]\n",
        "      V_target_min_batch = V_target[mini_batch_indices]\n",
        "      A_gae_min_batch = A_gae[mini_batch_indices]\n",
        "      #Actor update\n",
        "      #Use get_actor_loss with mini-batch data\n",
        "      #Add gradient clipping: nn.utils.clip_grad_norm_(actor.parameters(), 0.5)\n",
        "      actor_optimizer.zero_grad()\n",
        "      actor_loss = get_actor_loss(obs_min_batch,acts_min_batch, _, old_logp_min_batch, beta)\n",
        "      actor_loss.backward()\n",
        "      nn.utils.clip_grad_norm_(actor.parameters(), 0.5)#useful to not make gradients explode\n",
        "      actor_optimizer.step()\n",
        "      #Critic update\n",
        "      #Use mse loss with critic predictions and V_target\n",
        "      #Use .flatten() instead of .squeeze() for robust handling\n",
        "      #Add gradient clipping: nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
        "      critic_optimizer.zero_grad()\n",
        "      critic_loss = mse(critic(batch_obs).squeeze() ,V_target)\n",
        "      critic_loss.backward()\n",
        "      nn.utils.clip_grad_norm_(critic.parameters(), 0.5)\n",
        "      critic_optimizer.step()\n",
        "\n",
        "  return batch_rets, batch_lens"
      ],
      "metadata": {
        "id": "B36ehZOx2zyQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add PPO-specific hyperparameters and adjust the setup.\n",
        "\n",
        "Changes needed:\n",
        "1. Add: clip_ratio, n_ppo_epochs, mini_batch_size, beta\n",
        "2. Remove: n_critic_updates (no longer separate)\n",
        "3. Everything else stays the same!"
      ],
      "metadata": {
        "id": "RCohrMsQri2a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFIvWyc3vOG4",
        "outputId": "cfef2f29-d3e3-4652-8f34-5873615043a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████ | 908/1000 [1:53:19<11:22,  7.42s/it, avg_ret=-285, avg_len=132]"
          ]
        }
      ],
      "source": [
        "\n",
        "env_name='LunarLander-v3'\n",
        "hidden_sizes=[64, 64]\n",
        "lr=3e-4\n",
        "lr_critic=1e-3\n",
        "epochs=1_000\n",
        "batch_size=5_000\n",
        "gamma=0.99\n",
        "lam=0.95\n",
        "plot=True\n",
        "mini_batch_size = 128\n",
        "beta = 0.0\n",
        "n_ppo_epochs = 10\n",
        "env = gym.make(env_name, continuous=True) if env_name == \"LunarLander-v3\" else gym.make(env_name)  # BipedalWalker is continuous per default\n",
        "\n",
        "obs_dim = env.observation_space.shape[0]\n",
        "n_acts = env.action_space.shape[0]\n",
        "\n",
        "actor = mlp([obs_dim]+hidden_sizes+[2*n_acts])  # output-layer: 2*n_acts, for mean and logstd as the policy is stochastic\n",
        "critic = mlp([obs_dim]+hidden_sizes+[1])\n",
        "actor_optimizer = Adam(actor.parameters(), lr=lr)\n",
        "critic_optimizer = Adam(critic.parameters(), lr=lr_critic)\n",
        "mse = nn.MSELoss()\n",
        "\n",
        "returns = []\n",
        "std = []\n",
        "\n",
        "# training loop\n",
        "progress_bar = tqdm(range(1, epochs+1))\n",
        "for _ in progress_bar:\n",
        "  batch_rets, batch_lens = train_one_epoch()\n",
        "  avg_ret = np.mean(batch_rets)\n",
        "  avg_len = np.mean(batch_lens)\n",
        "  returns.append(avg_ret)\n",
        "  std.append(np.std(batch_rets))\n",
        "  progress_bar.set_postfix({\"avg_ret\": f\"{avg_ret:5.0f}\", \"avg_len\": f\"{avg_len:5.0f}\"})\n",
        "\n",
        "if plot:\n",
        "  plt.plot(returns)\n",
        "  plt.fill_between(range(len(returns)), np.array(returns) - np.array(std), np.minimum(300, np.array(returns) + np.array(std)), alpha=0.3)\n",
        "  plt.grid()\n",
        "  goal = {\"LunarLander-v3\": 200, \"BipedalWalker-v3\": 300}.get(env_name, 0)\n",
        "  plt.axhline(goal, color='r', linestyle='--')\n",
        "  plt.xlabel('epoch')\n",
        "  plt.ylabel('average return')\n",
        "  timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "  # TODO: Change filename\n",
        "  plt.savefig(f\"PPO_training_{timestamp}.png\")\n",
        "  plt.show()"
      ]
    }
  ]
}